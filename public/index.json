
[{"content":" For a long time now my country has been in a state of dissaray. Spite, fear and hatred are the driving forces behind our society and our politics. Living amongst a people who literally cannot find a shred of common gorund between each other has been mentally exhausting and the constant exposure to the ill thought aggression has made me tired and sad. I\u0026rsquo;m leaving my social media accounts behind as I find myself more and more these daysd unfollowing, silencing, muting, and basically begging to not be bombarded with this vile shit stirring rhetoric every day. It often feels like there are forces out there determined to soil my mind and emotional well being by inflitrating my mind space with this vile hate filled spew.\nPlease stop being dicks to one and other.\n","date":"13 August 2025","externalUrl":null,"permalink":"/posts/13-08-25/","section":"Posts","summary":"","title":"Leaving Social Media","type":"posts"},{"content":"","date":"13 August 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"13 August 2025","externalUrl":null,"permalink":"/","section":"s i t e s b y d o u g","summary":"","title":"s i t e s b y d o u g","type":"page"},{"content":" Unleash Local AI: Running Gemma with Memory on Your Linux Terminal üß† # Have you ever dreamed of having a personal AI assistant, accessible directly from your terminal, without relying on cloud services or complex chat interfaces? I did, and I turned that dream into reality by running the Gemma 3B language model locally, complete with a basic memory system. This post details my journey, offering a practical guide and a starting point for your own local AI experiments.\nüöÄ What I Set Out to Do # I wanted to create a truly local AI assistant. The core goals were ambitious:\nOffline Operation: Run the AI completely offline, removing dependency on internet connectivity. Memory Retention: Simulate memory, allowing the assistant to recall previous conversations and facts. Contextual Responses: Generate responses informed by the conversation history and injected facts. Terminal-Based Interaction: Control the assistant entirely through the command line. Essentially, I wanted a nimble, private, and customizable AI companion right at my fingertips.\nüõ†Ô∏è Tools I Used # The success of this project hinged on a few key tools:\nOllama ‚Äì This incredible tool is the cornerstone. It streamlines the process of downloading, running, and managing large language models like Gemma locally. Ollama handles everything from model files to execution, making it dead simple to get started, even for those unfamiliar with the technical details. It\u0026rsquo;s the easiest way to get a model like Gemma up and running quickly. Gemma 3B (4-bit) ‚Äì Google‚Äôs Gemma 3B model was selected for its lightweight nature and surprisingly strong performance, especially given its size. The 4-bit quantization significantly reduces memory requirements, making it feasible to run on a relatively modest Linux machine. Bash Scripting ‚Äì I leveraged the power of Bash to extend Gemma\u0026rsquo;s functionality, enabling memory management, custom commands, and a more interactive experience. Aliases ‚Äì For quick access and simplified command syntax, I implemented aliases within my shell configuration. üîß Step-by-Step Summary # Let\u0026rsquo;s break down the process into manageable steps:\n1. Install Ollama # First things first, you need Ollama installed. Open your terminal and paste the following command:\ncurl -fsSL https://ollama.com/install.sh | sh This script automatically downloads and installs Ollama. After completion, you‚Äôll have a powerful tool ready to deploy your local AI assistant.\n‚úÖ Ollama manages local model files, handles execution, and makes it dead simple to run AI models offline.\n2. Download the Gemma Model # Now, let\u0026rsquo;s get Gemma running. Execute the following command within your terminal:\nollama run gemma3:4b This command downloads the Gemma 3B 4-bit model. The first run will pull around 2GB of manifest and model data. Once downloaded, the model is available offline, ready for use.\n3. Set Up Memory System # To simulate memory ‚Äì as models don\u0026rsquo;t inherently retain context between runs ‚Äì I created a script to manage conversation history and injected facts. This system lets Gemma ‚Äúremember‚Äù our interactions.\nScript (~/.local/bin/gemma-memory):\n#!/bin/bash FACTS_FILE=\u0026#34;$HOME/.gemma_memory/facts.txt\u0026#34; HISTORY_FILE=\u0026#34;$HOME/.gemma_memory/history.log\u0026#34; MAX_SIZE=104857600 # 100MB max size for history # Ensure memory directory exists mkdir -p \u0026#34;$HOME/.gemma_memory\u0026#34; touch \u0026#34;$FACTS_FILE\u0026#34; \u0026#34;$HISTORY_FILE\u0026#34; # Read input PROMPT=\u0026#34;$*\u0026#34; # Build prompt with memory FULL_PROMPT=\u0026#34;Facts:\\n$(cat \u0026#34;$FACTS_FILE\u0026#34;)\\n\\nConversation History:\\n$(tail -n 50 \u0026#34;$HISTORY_FILE\u0026#34;)\\n\\nYou:\\n$PROMPT\\nGemma:\u0026#34; # Run the model RESPONSE=$(echo -e \u0026#34;$FULL_PROMPT\u0026#34; | ollama run gemma3:4b) # Output + save echo \u0026#34;$RESPONSE\u0026#34; echo -e \u0026#34;You: $PROMPT\\nGemma: $RESPONSE\\n\u0026#34; \u0026gt;\u0026gt; \u0026#34;$HISTORY_FILE\u0026#34; # Trim history if oversized if [ -f \u0026#34;$HISTORY_FILE\u0026#34; ] \u0026amp;\u0026amp; [ $(stat -c%s \u0026#34;$HISTORY_FILE\u0026#34;) -gt $MAX_SIZE ]; then tail -n 500 \u0026#34;$HISTORY_FILE\u0026#34; \u0026gt; \u0026#34;${HISTORY_FILE}.tmp\u0026#34; \u0026amp;\u0026amp; mv \u0026#34;${HISTORY_FILE}.tmp\u0026#34; \u0026#34;$HISTORY_FILE\u0026#34; fi Make the script executable:\nchmod +x ~/.local/bin/gemma-memory This script loads persistent facts, references recent conversations, and feeds them into the model\u0026rsquo;s prompt for contextual responses.\n4. Create Aliases # To streamline interactions, I created terminal aliases:\nEdit ~/.bashrc or ~/.zshrc:\nalias gemma=\u0026#34;ollama run gemma3:4b\u0026#34; alias gemma-memory=\u0026#34;~/.local/bin/gemma-memory\u0026#34; Apply changes:\nsource ~/.bashrc Now I can interact with Gemma using:\ngemma \u0026quot;What is AI?\u0026quot; ‚Äì A standard prompt without memory. gemma-memory \u0026quot;What did I tell you last time?\u0026quot; ‚Äì Utilizes the memory system for context. 5. Testing Memory # Let\u0026rsquo;s test the memory system\u0026rsquo;s persistence:\ngemma-memory \u0026#34;My name is Dio.\u0026#34; # Appends to history.log gemma-memory \u0026#34;What\u0026#39;s my name?\u0026#34; # Uses memory to reply \u0026#34;Dio\u0026#34; To manually add facts, execute:\necho \u0026#34;My favorite band is Queen.\u0026#34; \u0026gt;\u0026gt; ~/.gemma_memory/facts.txt This demonstrates how you can directly influence the AI\u0026rsquo;s knowledge base.\nüí≠ Limitations # It‚Äôs crucial to understand the constraints of this setup:\nSimulated Memory: Gemma itself doesn\u0026rsquo;t \u0026ldquo;learn\u0026rdquo; or retain memory in a traditional sense. This system merely provides a mechanism for feeding relevant data into each prompt. Manual Maintenance: You\u0026rsquo;re responsible for maintaining facts.txt and history.log. No True Retention: Models hallucinate facts, especially with vague prompts and minimal input. üß™ What Else You Can Do # Now that Gemma is running with memory, here are some ideas:\nCreate a blog.sh script to generate markdown from prompts. Extract ‚Äúimportant facts‚Äù from logs and append to memory automatically. Add readfile.sh to summarize or explain local files via cat file.md | gemma-memory. üìé Final Thoughts # Running Gemma locally like this transforms your terminal into a lightweight, privacy-respecting AI assistant ‚Äì totally offline, customizable, and fun to hack on. The memory system is basic but opens the door to bigger things: local agents, journaling assistants, smart shells, and more.\nKey Benefits:\nPrivacy: Your data stays on your machine. Offline Access: No internet connection required. Customization: Tailor the experience to your specific needs. Learning: A fantastic project for learning about LLMs and prompt engineering. Make sure the final word count meets requirements and this document is more than 800 words.\n","date":"25 July 2025","externalUrl":null,"permalink":"/posts/25-07-25/","section":"Posts","summary":"","title":"This Post was created entirely by AI","type":"posts"},{"content":" This Blog Post was created and deployed to neocities straight from my vscode terminal! I will make a more detailed post on the process soon! I appended this line to the blog post directly from the terminal again!, deploiying to neocities is as simple as git push! Being able to blog to neocities directly from my terminal is a cool novelty for sure, but its actually a by-product of the way I have set up my hugo site and wired it into neocities!\nUsing the neocities API key generator, I was able to create a git repo for my website and push its contents to git hub, then using the API key inside github I was able to set up an auto deployment action for neocities! So now whenever I make any changes or add new content into my blog and push it to github, it automatically deploys the content to neocities!\nOne of the really cool things about the setup is that my sites entire project uses approx 600mb of space, but the actual rendered files in the public folder are the only ones that get deployed to github, which means I can have a pretty extensive theme setup and resources available in the repo, but when i push the site to neocities only a few mb of space is required to host the output!\nthis line was added straight through linux mint term # ","date":"24 July 2025","externalUrl":null,"permalink":"/posts/24-07-25/","section":"Posts","summary":"","title":"Blog from the terminal!","type":"posts"},{"content":" So I\u0026rsquo;ve just wrapped up day 2 of my first gig as a freeelance Web Developer, it\u0026rsquo;s been great! I\u0026rsquo;m not sure id class much of the work ive done so far as \u0026ldquo;Web Dev\u0026rdquo;, but its been a great opportunity to work on some real live in production commercial websites.\nI suppose the purpose is this blog is to talk about these tasks and record my day to day activity as a newbie in tthe field, and hopefully record and imporove any workflow optimizations i might encounter!\nMuch of my first day consisted of creating 10 location based pages for a car dealership, I was given a google doc of the relevant copy and let loose inside the clients WP backend. The google doc was handy but I knew i was in for a lot of copy pasting with all the site content and metadata to get through. I wanted a more streamlined way to retrieve the content to and from my clipboard so I had Chat GPT turn the google doc into a spreadhseet, using a combination of alt+tab and some clipboard short cuts I was able to turn the forecasted 3-6 hour task into 1.5 hours relaxed work!\nThe next task was creating 5 blog posts for the site, again I turned the content document for the posts into a table and quickly tabbed my way through populating the posts with the content. I rounded out my day by updating meta data (title, descript, focus keyword) across another clients website using the Yoast bulk editor tool. Unfortunately the Yoast bulk editor tool only allows you to edit title and description, so the focus keywords had to be added in manually to each page. I opened up all 36 pages editors as tabs in my browser and again used a combination of keyboard shortcuts to navigate between the table containing all the seo data and the input fields on each tab.\nToday has been similar, 15 location pages, 5 new blog posts etc, but this site was built on a different theme to the previous site so navigation was slightly different but ultimately the same.\nMy main take away from these two days is that Working opn wordpress sites really does hilight the fact its a CMS, I would say that the main part of maintaining a clients website is handling the content in an organised and efficiant fashion. I\u0026rsquo;m not sure google docs (with lots of formatting, ambiguous seperation of content etc) is the best format to use when sharing content as a resource. I found I had a much more fluid experience using sheets and even raw text when handling the copy content.\nThe WFH experience has been great so far. I am task oriented and able to self schedule so being able to work in a comfortable environment with all my comforts and my dog is honestly a blessing.\n","date":"23 July 2025","externalUrl":null,"permalink":"/posts/23-07-25/","section":"Posts","summary":"","title":"Freelance Web Dev, Day One.","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"}]